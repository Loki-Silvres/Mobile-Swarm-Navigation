# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AoL46wr8rTCzr1znewqTGW8Kjez7an04
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load the model and tokenizer only once
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def generate_response(prompt):
    messages = [
        {"role": "system", "content": r"""
       WE are Manobots 4 swarm of bots managing the warehouse named Alok-Packagings Pvt. Ltd. and Mr. Alokraj is the owner of this
    organization.

    Alok Packaging Private Ltd is your trusted partner for all your packaging needs.
    The warehouse currently has various packages; their traking IDs  should be asked before processing and intent is status.
    if tracking id is not given reply that the warehouse has tons of packages and can't help you without the tracking id.
    in case of fire , flood or any natural disaster the intent is emergency and people has to move towards emergency exit location of   and the intent is emergency
    In case of fire, smoke, or burning, the locations of fire extinguishers are as follows:
    1) Near block A (12, 32, 442)
    2) Near block B (123, 12, 442)
    3) Near block D (122, 31, 123)
    and intent is emergency , fire.
     if the bot is asked to do some task like switching on off the light  or fan or any switch .switch id shpuld be asked and  the intent should be the switch.
     other tasks have intent conversation
intent should be displayed at each task i specified the intent in format @#inten#@
        """},
        {"role": "user", "content": prompt}
    ]

    # Construct the input manually
    text = ""
    for message in messages:
        text += f"{message['role']}: {message['content']}\n"
    text += "assistant:"

    # Tokenize input
    model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

    # Generate output
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512
    )

    # Decode generated text
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return response

# Interactive loop for continuous input
while True:
    user_input = input("Enter your prompt (type 'exit' to quit): ")
    if user_input.lower() == "exit":
        print("Exiting the loop. Goodbye!")
        break
    response = generate_response(user_input)
    print("\nAssistant Response:\n", response)